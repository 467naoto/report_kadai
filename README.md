# 大規模言語モデル　課題

## 1. 自分が扱ってみたい単純な自然言語処理タスクを１つ選定し、選んだタスクについて、使用可能な事前学習済みモデルを１つ選べ、モデル名・入力形式・用途・モデル規模（パラメータ数）・事前学習データ量・訓練過程を表にまとめよ

### ニュースの見出しの分類

| モデル名 | 入力形式 | 用途 | モデル規模 | 事前学習データ量 | 訓練過程 |
|-----------|------------|--------|--------------|------------------|------------|
| cl-tohoku/bert-base-japanese-v3 | 日本語テキストをトークナイザーでサブワードに分割して入力、最大長512トークン | 文章分類、固有表現抽出 | 約110Mパラメータ（層数:12、隠れ次元:768、Attentionヘッド数:12） | 約20GBの日本語テキスト | MLM、NSP、AdamW |

<br>
<br>
    
## 2. プロンプト制御・文脈内学習・CoTのうち１つを、１で選んだタスクに当てはめ、「どう使えそうか」「どう改善が期待できるか」について仮説を立て説明せよ

#### プロンプト制御を当てはめるとする

### どう使えそうか
ニュースの見出しをカテゴリに分類する際に文脈的理解を促すことができると考える

### どう改善が期待できるか
指示（プロンプト）を入力文に付与することで、曖昧なニュースの見出しがあった場合に対する文脈理解の向上が見込めると考える


## 3.アライメント/無害性/指示チューニングといった観点から、１で選んだタスクにおける「リスク（誤答、バイアス、無害性違反など）」を１つ挙げ、それに対してどのような対策が考えられるかを説明せよ

ニュース見出し分類では、事前学習データに含まれる社会的・政治的バイアスが残存し、特定の主体（政党・企業・人物など）を含む見出しが特定カテゴリに自動的に寄ってしまうリスクがある。これは誤分類だけでなく、偏った解釈を助長する点でアライメント上の問題となる。対策としては、まず バイアス分析（誤分類例の収集、特定語の出現と分類の相関の可視化）を行い、どの語彙が判断に強く影響しているか特定することが重要である。その上で、①データ拡張により多様な文脈を追加しバイアスを薄める、②モデルに中立的判断を促す プロンプト指示の工夫（「固有名詞に依存せず内容全体で判断せよ」などを挙げることができる。

## 4.１で選んだタスクに対して、軽量ファインチューニングを行え。例えば、少数データ（100~500件）を準備して、モデルをファインチューニングし、その前後での性能を比較せよ。さらに「どこでうまくいったか/うまくいかなかったか」を整理し、エラー分析的な視点から「モデルのショートカット」「誤分類の傾向」などを簡潔にまとめよ

### ファインチューニング前後の結果
| 比較項目 | ファインチューニング前 | ファインチューニング後 |
| ------- | ---------------- | ---------------- |
| Accuracy| 0.5 | 0.0 |
| Loss | 1.55 | 1.768 |

### うまくいった点
モデルの事前学習能力により、学習前でも一定の分類性能が得られた<br>
少量データでも学習が実行できる実験環境が構築できた

### うまくいかなかった点
ファインチューニング後の精度が 0% となり、性能が大幅に悪化<br>
LoRA の適用層がモデルの構造と一致しておらず、学習が破綻した可能性がある

### 誤分類の傾向
すべての見出しを特定の1クラスに偏って予測している可能性が高い


## 5.軽量化・効率化（例：バッチサイズ、学習率、勾配累積/LoRAなど）について、今後１で選んだタスクに適用することで改善が期待される工夫を1〜２案挙げ、理由とともに説明せよ
### 1.LoRAの導入
※LoRA：LoRAではモデル全体ではなく一部の重みだけを低ランク行列で更新することで、学習パラメータを大幅に削減する手法
#### 理由
少量データの場合、全パラメータ更新だと過学習しやすいが、LoRAなら汎化性能を維持しつつタスク適応が可能なため
#### 期待される改善点
- 全パラメータ更新よりも過学習しづらく、少量データに相性がいい
- 学習に必要なVRAM/メモリ消費が減り、Colab環境でも高速に学習可能である

### 2.勾配累積+小バッチサイズ
GPUメモリに余裕がない状態でも複数バッチを累積してから1回の更新を行い、実質的にバッチサイズを大きくする
#### 理由
ニュースの見出し分類では、文鳥が短いこともあり、バッチサイズを増やしてもメモリに収まりやすいため
#### 期待される改善点
- 安定した勾配更新が可能になり、精度のブレが減る
- 少量データでも安定して収束しやすい

### まとめ
| 手法 | 期待される改善 | 改善によって良くなる点 |
| --- | ------------ | ---------------- |
| LoRA | 高速、省メモリ、過学習抑制、精度向上 | 少量データのファインチューニング（軽量ファインチューニング） |
| 勾配累積 | 安定した学習、精度向上、メモリ節約 | 小規模環境での精度改善 |


